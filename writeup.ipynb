{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Let's consider Multi-Agent Multi-Arm Bandit problem where there are $N$ agents (indexed by $i=1,2,...,N$) and $M$ bandit arms (indexed by $k=1,2,...,M$).\n",
    "At each timestep $t=1,2,...,T$, each agent $i$ takes an action $a_{i,t} \\in \\{1,2,...,M\\}$, pulls arm $k = a_{i,t}$ and receives a reward $r_{i,t} \\sim p_k$. \n",
    "Agents share their estimated reward distributions $\\hat{p}_k$ with each other according to a network with the adjacency matrix $A = \\{w_{i,j} \\}$. \n",
    "\n",
    "The goal of each agent is to maximize its total expected reward over horizon $T$.\n",
    "\\begin{equation}\n",
    "    \\pi_i^* = \\underset{\\pi_i}{\\text{argmax }}  \\mathbb{E}_{\\substack{a_{i,t} \\sim \\pi_i \\\\ r_{i,t} \\sim p_{a_{i,t}}}} \\sum_{t=1}^T r_{i,t}\n",
    "\\end{equation}\n",
    "\n",
    "## Assumptions\n",
    "- The reward distribution $p_k$ of each arm $k$ is unknown to the agents.\n",
    "- The reward distributions $p_k$ are independent of each other.\n",
    "- The reward distributions $p_k$ are stationary.\n",
    "- elements of the adjacency matrix $w_{i,j}$ are either 0 or 1 where $w_{i,j} = 1$ means that agent $i$ can share its estimated reward distribution with agent $j$ and $w_{i,j} = 0$ means that agent $i$ cannot share its estimated reward distribution with agent $j$.\n",
    "- Reward distributions are shared between agents by means of its mean, i.e., $\\hat{\\mu}_{i,k,t} = \\sum_j \\hat{\\mu}_{j,k,t}$ for all $j$ such that $w_{i,j} = 1$.\n",
    "- The reward distributions are Gaussian, i.e., $p_k = \\mathcal{N}(\\mu_k,\\sigma_k)$.\n",
    "\n",
    "- Agent policies are determined by the Upper Confidence Bound (UCB1) algorithm:\n",
    "\\begin{equation}\n",
    "    a_{i,t} = \\pi_i(t) = \\underset{k}{\\text{argmax }} \\left[ \\hat{\\mu}_{i,k,t} + \\sqrt{\\frac{2\\log t}{n_{i,k,t}}} \\right]\n",
    "\\end{equation}\n",
    "where $n_{i,k,t}$ is the number of times agent $i$ has pulled arm $k$ up to time $t$ and $\\hat{\\mu}_{i,k,t}$ is the empirical mean of rewards received by agent $i$ when it pulls arm $k$ up to time $t$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
