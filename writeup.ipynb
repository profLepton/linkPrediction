{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Let's consider Multi-Agent Multi-Arm Bandit problem where there are $N$ agents (indexed by $i=1,2,...,N$) and $M$ bandit arms (indexed by $j=1,2,...,M$).\n",
    "At each timestep $t=1,2,...,T$, each agent $i$ takes an action $a_{i,t} \\in \\{1,2,...,M\\}$, pulls arm $j = a_{i,t}$ and receives a reward $r_{i,t} \\sim p_j$. Agents share their estimated reward distributions $\\hat{p}_j$ with each other over a network $G$. The goal of each agent is to maximize its total expected reward over horizon $T$.\n",
    "\\begin{equation}\n",
    "    \\pi_i^* = \\underset{\\pi_i}{\\text{argmax }}  \\mathbb{E}_{\\substack{a_{i,t} \\sim \\pi_i \\\\ r_{i,t} \\sim p_{a_{i,t}}}} \\sum_{t=1}^T r_{i,t}\n",
    "\\end{equation}\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "- The reward distribution $p_j$ of each arm $j$ is unknown to the agents.\n",
    "- The reward distributions $p_j$ are independent of each other.\n",
    "- The reward distributions $p_j$ are stationary.\n",
    "- The reward distributions are Gaussian, i.e., $p_j = \\mathcal{N}(\\mu_j,\\sigma_j)$.\n",
    "- Agent policies are determined by the Upper Confidence Bound (UCB1) algorithm:\n",
    "\\begin{equation}\n",
    "    a_{i,t} = \\pi_i(t) = \\underset{a_{i,t}}{\\text{argmax }} \\left[ \\hat{\\mu}_{i,t}(a_{i,t}) + \\sqrt{\\frac{2\\log t}{n_{i,t}(a_{i,t})}} \\right]\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
